template:
  id: "bmad.qa.test-strategy.v1"
  name: "QA â€” Test Strategy"
sections:
  - id: "feature_overview"
    title: "Feature Overview"
    description: "What's being tested. Example: 'AI-powered task prioritization feature. Users see ML-generated priority suggestions. Real-time updates. Multiple algorithm options.'"
  - id: "test_objectives"
    title: "Test Objectives"
    description: "What testing aims to achieve. Example: 'Objectives: Validate core functionality, ensure performance <200ms, verify algorithm accuracy, test edge cases, confirm accessibility compliance.'"
  - id: "test_scope"
    title: "Test Scope"
    description: "What's in and out of scope. Example: 'In scope: UI, API, ML integration, real-time updates, error handling. Out of scope: ML model training, load testing >1000 users, old browser versions.'"
  - id: "test_levels"
    title: "Test Levels"
    description: "Testing layers and approach. Example: 'Unit tests: Business logic, data transformations (80% coverage). Integration: API + ML service. E2E: User workflows. Performance: Response times. Accessibility: WCAG 2.1 AA.'"
  - id: "test_types"
    title: "Test Types & Techniques"
    description: "Testing methods to use. Example: 'Functional: Positive/negative testing, boundary values. Non-functional: Performance, usability, security. Exploratory: Edge cases, error scenarios. Regression: Existing features.'"
  - id: "test_environment"
    title: "Test Environment"
    description: "Where testing will occur. Example: 'Environments: Local dev, staging (prod-like), beta (real users). Browsers: Chrome, Firefox, Safari, Edge (latest 2 versions). Devices: Desktop, mobile, tablet.'"
  - id: "test_data"
    title: "Test Data Strategy"
    description: "Data needed for testing. Example: 'Test data: Seeded user accounts (small/medium/large task lists), synthetic ML responses, edge cases (empty, 1000+ tasks), various priority scenarios.'"
  - id: "acceptance_criteria"
    title: "Exit Criteria"
    description: "When testing is complete. Example: 'Exit criteria: All test cases passed, >80% code coverage, no P0/P1 defects, performance benchmarks met, accessibility audit passed, stakeholder approval.'"
  - id: "risks"
    title: "Test Risks & Mitigation"
    description: "Testing risks and how to address. Example: 'Risk: ML service unreliable. Mitigation: Mock service for most tests. Risk: Test data quality. Mitigation: Use prod data anonymized. Risk: Time constraints. Mitigation: Prioritize critical paths.'"
metadata:
  usage: "test-planning"
  priority: "high"
  workflow: "strategy"
